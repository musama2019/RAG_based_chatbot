{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP7_dzUZTCiV"
      },
      "outputs": [],
      "source": [
        "!pip install langchain[all]\n",
        "!pip install faiss-cpu\n",
        "!pip install openai\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install gradio\n",
        "!pip install unstructured\n",
        "!pip install InstructorEmbedding\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afAH828BV-Uf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRlUxrnuZbOi"
      },
      "source": [
        "#1. Load the documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkNM7MsEWBMn"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "loader = DirectoryLoader('path', glob=\"*.txt\")\n",
        "loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ESE-JjcYHlw"
      },
      "outputs": [],
      "source": [
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dyLGAS2YM8I"
      },
      "outputs": [],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5va1oW1AZVvu"
      },
      "source": [
        "#2. Transform the documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDuDwJ3IZYCQ"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 3500,\n",
        "    chunk_overlap  = 500,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OI_00njan-Q"
      },
      "outputs": [],
      "source": [
        "texts =text_splitter.split_documents(docs)\n",
        "print(texts[0])\n",
        "print(texts[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-YeV7T0bbAM"
      },
      "outputs": [],
      "source": [
        "len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONbSOQP8b76v"
      },
      "source": [
        "# 3. Generate the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnxCiig4bsYs"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAk4rwGjdmQx"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = 'Your OpenAI-api key'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRDjxZfLgGtm"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD3w5VEagKSb"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sunb417XieYr"
      },
      "source": [
        "#4. Store it in vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXwgGznwfuZ-"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77MhITQVxFZL"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGkdqQn_g-Ty"
      },
      "outputs": [],
      "source": [
        "vectors= FAISS.from_documents(texts, embeddings )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5K0MCZdhPOo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a57035-791f-46e6-cb68-c3f8c35cc2dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x7befcaadece0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD7yDuEchSsh"
      },
      "outputs": [],
      "source": [
        "#  The data is strore in the local folder. Use the path\n",
        "vectors.save_local('path')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svXY9BRvoIfR"
      },
      "source": [
        "##Chains\n",
        "\n",
        "● LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp7itu-VoLlK"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eXVzCkxoSL6"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POr0IE04sEi7"
      },
      "source": [
        "● Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVeT6wVxoUN5"
      },
      "outputs": [],
      "source": [
        "! pip install langchain openai chromadb langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i4UihcXsX07"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "print(rag_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kYQNdtMYnVy"
      },
      "source": [
        "● Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNUYSCRZtDZI"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key= 'answer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNGstAfPcYE0"
      },
      "source": [
        "● Retrievers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqPnVyI0cZk1"
      },
      "outputs": [],
      "source": [
        "retriever = vectors.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ie310fgEm5"
      },
      "source": [
        "Now pass all the above LLm, retreval, Memory and prompt to the RAG chain Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_o0kbS0eeOz"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "rag_chain= (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | chat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8imMttxmgpGI"
      },
      "outputs": [],
      "source": [
        "chain= rag_chain.invoke(\"What is Task Decompositon\")\n",
        "chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z9fzX79kzdh"
      },
      "source": [
        "##Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDs9_OyMg402"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "def chatllama(user_msg, history):\n",
        "  result = rag_chain.invoke(user_msg )\n",
        "  return result.content\n",
        "\n",
        "# Add a title to the interface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chatllama,\n",
        "    title=\"                          Chatbot                \",\n",
        ")\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N7diTt4HOah"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}